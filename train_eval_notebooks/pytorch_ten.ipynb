{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Edit Networks\n",
    "\n",
    "This notebook contains applications of graph edit networks (without edge actions) to trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 636/100000 [00:00<00:15, 6357.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now running for 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:19<00:00, 5239.96it/s]\n",
      "  0%|          | 352/100000 [00:00<00:28, 3485.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now running for 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:46<00:00, 2135.14it/s]\n",
      "  0%|          | 296/100000 [00:00<01:06, 1509.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now running for 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:48<00:00, 922.33it/s]\n",
      "  0%|          | 149/100000 [00:00<01:08, 1460.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now running for 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [02:24<00:00, 691.57it/s]\n",
      "  0%|          | 71/100000 [00:00<02:20, 709.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now running for 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [05:29<00:00, 303.42it/s]\n",
      "  0%|          | 31/100000 [00:00<05:45, 289.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now running for 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [06:39<00:00, 250.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../boolean_formulae')\n",
    "sys.path.append('../peano_addition')\n",
    "\n",
    "import boolean_formulae\n",
    "import peano_addition\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "lens = {3 : [], 4 : [], 5 : []}\n",
    "bools3 = []\n",
    "bools3_values = []\n",
    "for l in range(3,6):\n",
    "    print(f'now running for {l}')\n",
    "    for i in tqdm(range(100000)):\n",
    "        ts = boolean_formulae.generate_time_series(l)\n",
    "        ll = len(ts)\n",
    "        lens[l].append(ll )\n",
    "        if ll>= 2:\n",
    "            if ts[0] not in bools3:\n",
    "                bools3.append(ts[0])\n",
    "                bools3_values.append(ts)\n",
    "\n",
    "df = pd.DataFrame(lens)\n",
    "melted = pd.melt(df)\n",
    "lens = {3 : [], 4 : [], 5 : []}\n",
    "\n",
    "melted['kind'] = ['boolean'] * len(melted['variable'])\n",
    "pean3 = []\n",
    "pean3_values = []\n",
    "for l in range(3,6):\n",
    "    print(f'now running for {l}')\n",
    "    for i in tqdm(range(100000)):\n",
    "        ts = peano_addition.generate_time_series(l)\n",
    "        ll = len(ts)\n",
    "        lens[l].append(ll )\n",
    "        if ll>= 2:\n",
    "            if ts[0] not in pean3:\n",
    "                pean3.append(ts[0])\n",
    "                pean3_values.append(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(lens)\n",
    "melted2 = pd.melt(df)\n",
    "melted2['kind'] = ['peano'] * len(melted2['variable'])\n",
    "\n",
    "melted3 = pd.concat([melted, melted2])\n",
    "melted3.to_csv('penosBooleans.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We were able to find 77351 Peano graphs, which is 25.78% of the sampled series\n",
      "We were able to find 38094 boolaen graphs, which is 12.70% of the sampled series\n"
     ]
    }
   ],
   "source": [
    "print(f'We were able to find {len(pean3)} Peano graphs, which is {len(pean3)*100/300000 :.2f}% of the sampled series')\n",
    "print(f'We were able to find {len(bools3)} boolaen graphs, which is {len(bools3)*100/300000 :.2f}% of the sampled series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('results_temp/bool_timseries.pkl', 'wb') as f:\n",
    "    pickle.dump(bools3_values, f)\n",
    "\n",
    "with open('results_temp/peano_timseries.pkl', 'wb') as f:\n",
    "    pickle.dump(pean3_values, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38094\n"
     ]
    }
   ],
   "source": [
    "with open('results_temp/bool_timseries.pkl', 'rb') as f:\n",
    "    ALL_BOOLS = pickle.load(f)\n",
    "\n",
    "with open('results_temp/peano_timseries.pkl', 'rb') as f:\n",
    "    ALL_PEANOS = pickle.load(f)\n",
    "\n",
    "print(len(ALL_BOOLS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Boolean Formula Simplification\n",
    "\n",
    "We generate a random Boolean formula over the variables $x$ and $y$ with at most 3 binary operators and then apply simplification rules until none apply anymore. The time series is the series of iteratively simpler formulae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_tree_edit_networks as ten\n",
    "\n",
    "# the number of experimental repititions\n",
    "R = 5\n",
    "# the number of test trees\n",
    "N_test = 10\n",
    "\n",
    "# training hyperparameters\n",
    "max_epochs     = 12\n",
    "learning_rate  = 1E-3\n",
    "weight_decay   = 1E-5\n",
    "loss_threshold = 1E-3\n",
    "\n",
    "# model hyperparameters\n",
    "num_layers = 2\n",
    "dim_hid = 64\n",
    "skip_connections = False\n",
    "nonlin = torch.nn.ReLU()\n",
    "max_degree = 10\n",
    "\n",
    "accs = np.zeros(R)\n",
    "learning_curves = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lst = pd.read_pickle(r'results_temp/bool_timseries.pkl')\n",
    "max_epochs = len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- repeat 1 of 5 ---\n",
      "loss avg after 100 epochs: 20.0979\n",
      "loss avg after 200 epochs: 6.90118\n",
      "loss avg after 300 epochs: 5.03764\n",
      "loss avg after 400 epochs: 4.29327\n",
      "loss avg after 500 epochs: 3.16986\n",
      "loss avg after 600 epochs: 2.53963\n",
      "loss avg after 700 epochs: 2.3111\n",
      "loss avg after 800 epochs: 2.274\n",
      "loss avg after 900 epochs: 2.08182\n",
      "loss avg after 1000 epochs: 1.79584\n",
      "loss avg after 1100 epochs: 1.6507\n",
      "loss avg after 1200 epochs: 0.97665\n",
      "loss avg after 1300 epochs: 1.25077\n",
      "loss avg after 1400 epochs: 0.765624\n",
      "loss avg after 1500 epochs: 0.569523\n",
      "loss avg after 1600 epochs: 0.444536\n",
      "loss avg after 1700 epochs: 0.417973\n",
      "loss avg after 1800 epochs: 0.367995\n",
      "loss avg after 1900 epochs: 0.249243\n",
      "loss avg after 2000 epochs: 0.286846\n",
      "loss avg after 2100 epochs: 0.228351\n",
      "loss avg after 2200 epochs: 0.164747\n",
      "loss avg after 2300 epochs: 0.137236\n",
      "loss avg after 2400 epochs: 0.162831\n",
      "loss avg after 2500 epochs: 0.151879\n",
      "loss avg after 2600 epochs: 0.125393\n",
      "loss avg after 2700 epochs: 0.0907277\n",
      "loss avg after 2800 epochs: 0.0657102\n",
      "loss avg after 2900 epochs: 0.0675767\n",
      "loss avg after 3000 epochs: 0.0382633\n",
      "loss avg after 3100 epochs: 0.0489232\n",
      "loss avg after 3200 epochs: 0.0477691\n",
      "loss avg after 3300 epochs: 0.0746102\n",
      "loss avg after 3400 epochs: 0.0597627\n",
      "loss avg after 3500 epochs: 0.173052\n",
      "loss avg after 3600 epochs: 0.0572195\n",
      "loss avg after 3700 epochs: 0.111292\n",
      "loss avg after 3800 epochs: 0.0471025\n",
      "loss avg after 3900 epochs: 0.0745517\n",
      "loss avg after 4000 epochs: 0.0421077\n",
      "loss avg after 4100 epochs: 0.0252383\n",
      "loss avg after 4200 epochs: 0.0345564\n",
      "loss avg after 4300 epochs: 0.0353574\n",
      "loss avg after 4400 epochs: 0.0275224\n",
      "loss avg after 4500 epochs: 0.0219494\n",
      "loss avg after 4600 epochs: 0.0145424\n",
      "loss avg after 4700 epochs: 0.0288456\n",
      "loss avg after 4800 epochs: 0.00750506\n",
      "loss avg after 4900 epochs: 0.0143463\n",
      "loss avg after 5000 epochs: 0.0104498\n",
      "loss avg after 5100 epochs: 0.185097\n",
      "loss avg after 5200 epochs: 0.0108483\n",
      "loss avg after 5300 epochs: 0.00884866\n",
      "loss avg after 5400 epochs: 0.00829031\n",
      "loss avg after 5500 epochs: 0.0409241\n",
      "loss avg after 5600 epochs: 0.0914641\n",
      "loss avg after 5700 epochs: 0.0160946\n",
      "loss avg after 5800 epochs: 0.00482822\n",
      "loss avg after 5900 epochs: 0.0064101\n",
      "loss avg after 6000 epochs: 0.00411865\n",
      "loss avg after 6100 epochs: 0.00320245\n",
      "loss avg after 6200 epochs: 0.0128799\n",
      "loss avg after 6300 epochs: 0.182141\n",
      "loss avg after 6400 epochs: 0.0422867\n",
      "loss avg after 6500 epochs: 0.0129567\n",
      "loss avg after 6600 epochs: 0.00786849\n",
      "loss avg after 6700 epochs: 0.00361513\n",
      "loss avg after 6800 epochs: 0.0129139\n",
      "loss avg after 6900 epochs: 0.00260869\n",
      "loss avg after 7000 epochs: 0.0183718\n",
      "loss avg after 7100 epochs: 0.00281936\n",
      "loss avg after 7200 epochs: 0.00416444\n",
      "loss avg after 7300 epochs: 0.00256321\n",
      "loss avg after 7400 epochs: 0.00489287\n",
      "loss avg after 7500 epochs: 0.00953734\n",
      "loss avg after 7600 epochs: 0.00175998\n",
      "loss avg after 7700 epochs: 0.0016182\n",
      "loss avg after 7800 epochs: 0.00184315\n",
      "loss avg after 7900 epochs: 0.00163391\n",
      "loss avg after 8000 epochs: 0.00128092\n",
      "loss avg after 8100 epochs: 0.867731\n",
      "loss avg after 8200 epochs: 0.0307755\n",
      "loss avg after 8300 epochs: 0.010676\n",
      "loss avg after 8400 epochs: 0.00465676\n",
      "loss avg after 8500 epochs: 0.00516489\n",
      "loss avg after 8600 epochs: 0.00405553\n",
      "loss avg after 8700 epochs: 0.00223955\n",
      "loss avg after 8800 epochs: 0.00174206\n",
      "loss avg after 8900 epochs: 0.00271767\n",
      "loss avg after 9000 epochs: 0.00234749\n",
      "accuracy: 1\n",
      "--- repeat 2 of 5 ---\n",
      "loss avg after 100 epochs: 21.5149\n",
      "loss avg after 200 epochs: 7.52767\n",
      "loss avg after 300 epochs: 4.16781\n",
      "loss avg after 400 epochs: 3.63238\n",
      "loss avg after 500 epochs: 2.87606\n",
      "loss avg after 600 epochs: 2.61456\n",
      "loss avg after 700 epochs: 2.60798\n",
      "loss avg after 800 epochs: 2.27016\n",
      "loss avg after 900 epochs: 1.90416\n",
      "loss avg after 1000 epochs: 1.73672\n",
      "loss avg after 1100 epochs: 1.10632\n",
      "loss avg after 1200 epochs: 1.14505\n",
      "loss avg after 1300 epochs: 0.947897\n",
      "loss avg after 1400 epochs: 0.925387\n",
      "loss avg after 1500 epochs: 0.590532\n",
      "loss avg after 1600 epochs: 0.527634\n",
      "loss avg after 1700 epochs: 0.437065\n",
      "loss avg after 1800 epochs: 0.449995\n",
      "loss avg after 1900 epochs: 0.374965\n",
      "loss avg after 2000 epochs: 0.348528\n",
      "loss avg after 2100 epochs: 0.342973\n",
      "loss avg after 2200 epochs: 0.241536\n",
      "loss avg after 2300 epochs: 0.363174\n",
      "loss avg after 2400 epochs: 0.347341\n",
      "loss avg after 2500 epochs: 0.297897\n",
      "loss avg after 2600 epochs: 0.295853\n",
      "loss avg after 2700 epochs: 0.181916\n",
      "loss avg after 2800 epochs: 0.251574\n",
      "loss avg after 2900 epochs: 0.121192\n",
      "loss avg after 3000 epochs: 0.178674\n",
      "loss avg after 3100 epochs: 0.124488\n",
      "loss avg after 3200 epochs: 0.109491\n",
      "loss avg after 3300 epochs: 0.0779986\n",
      "loss avg after 3400 epochs: 0.0739776\n",
      "loss avg after 3500 epochs: 0.0775261\n",
      "loss avg after 3600 epochs: 0.0561943\n",
      "loss avg after 3700 epochs: 0.0632443\n",
      "loss avg after 3800 epochs: 0.0533135\n",
      "loss avg after 3900 epochs: 0.642881\n",
      "loss avg after 4000 epochs: 0.0894164\n",
      "loss avg after 4100 epochs: 0.0885614\n",
      "loss avg after 4200 epochs: 0.0671221\n",
      "loss avg after 4300 epochs: 0.0459569\n",
      "loss avg after 4400 epochs: 0.0738375\n",
      "loss avg after 4500 epochs: 0.112489\n",
      "loss avg after 4600 epochs: 0.203371\n",
      "loss avg after 4700 epochs: 0.0901022\n",
      "loss avg after 4800 epochs: 0.0488925\n",
      "loss avg after 4900 epochs: 0.0565991\n",
      "loss avg after 5000 epochs: 0.0362326\n",
      "loss avg after 5100 epochs: 0.014335\n",
      "loss avg after 5200 epochs: 0.0799347\n",
      "loss avg after 5300 epochs: 0.0199444\n",
      "loss avg after 5400 epochs: 0.0256356\n",
      "loss avg after 5500 epochs: 0.0165471\n",
      "loss avg after 5600 epochs: 0.0371678\n",
      "loss avg after 5700 epochs: 0.0330443\n",
      "loss avg after 5800 epochs: 0.0177959\n",
      "loss avg after 5900 epochs: 0.115536\n",
      "loss avg after 6000 epochs: 0.0281877\n",
      "loss avg after 6100 epochs: 0.00816116\n",
      "loss avg after 6200 epochs: 0.0371226\n",
      "loss avg after 6300 epochs: 0.024919\n",
      "loss avg after 6400 epochs: 0.0339087\n",
      "loss avg after 6500 epochs: 0.00716465\n",
      "loss avg after 6600 epochs: 0.037687\n",
      "loss avg after 6700 epochs: 0.0133353\n",
      "loss avg after 6800 epochs: 0.00627853\n",
      "loss avg after 6900 epochs: 0.0062496\n",
      "loss avg after 7000 epochs: 0.00300078\n",
      "loss avg after 7100 epochs: 0.0171361\n",
      "loss avg after 7200 epochs: 0.00863657\n",
      "loss avg after 7300 epochs: 0.533637\n",
      "loss avg after 7400 epochs: 0.106846\n",
      "loss avg after 7500 epochs: 0.0152301\n",
      "loss avg after 7600 epochs: 0.0100225\n",
      "loss avg after 7700 epochs: 0.0126855\n",
      "loss avg after 7800 epochs: 0.00463436\n",
      "loss avg after 7900 epochs: 0.042558\n",
      "loss avg after 8000 epochs: 0.00364873\n",
      "loss avg after 8100 epochs: 0.0120355\n",
      "loss avg after 8200 epochs: 0.00826367\n",
      "loss avg after 8300 epochs: 0.00169826\n",
      "loss avg after 8400 epochs: 0.00923248\n",
      "loss avg after 8500 epochs: 0.0016725\n",
      "loss avg after 8600 epochs: 0.00252411\n",
      "expected tree root(or(True, or(not_y, not_y), y)) but was actually root(True, or(not_y, not_y), y)\n",
      "expected tree root(or(True, not_y, not_y)) but was actually root(True, not_y, not_y)\n",
      "expected tree root(False) but was actually root\n",
      "accuracy: 0.863636\n",
      "--- repeat 3 of 5 ---\n",
      "loss avg after 100 epochs: 20.6404\n",
      "loss avg after 200 epochs: 8.20622\n",
      "loss avg after 300 epochs: 4.34863\n",
      "loss avg after 400 epochs: 3.24553\n",
      "loss avg after 500 epochs: 3.1514\n",
      "loss avg after 600 epochs: 2.2595\n",
      "loss avg after 700 epochs: 1.98933\n",
      "loss avg after 800 epochs: 1.86895\n",
      "loss avg after 900 epochs: 1.18915\n",
      "loss avg after 1000 epochs: 1.11697\n",
      "loss avg after 1100 epochs: 0.857104\n",
      "loss avg after 1200 epochs: 0.965019\n",
      "loss avg after 1300 epochs: 0.664764\n",
      "loss avg after 1400 epochs: 0.482283\n",
      "loss avg after 1500 epochs: 0.611469\n",
      "loss avg after 1600 epochs: 0.807681\n",
      "loss avg after 1700 epochs: 0.497109\n",
      "loss avg after 1800 epochs: 0.298546\n",
      "loss avg after 1900 epochs: 0.256424\n",
      "loss avg after 2000 epochs: 0.300275\n",
      "loss avg after 2100 epochs: 0.25262\n",
      "loss avg after 2200 epochs: 0.204337\n",
      "loss avg after 2300 epochs: 0.168058\n",
      "loss avg after 2400 epochs: 0.252874\n",
      "loss avg after 2500 epochs: 0.266221\n",
      "loss avg after 2600 epochs: 0.13512\n",
      "loss avg after 2700 epochs: 0.130264\n",
      "loss avg after 2800 epochs: 0.15439\n",
      "loss avg after 2900 epochs: 0.0884093\n",
      "loss avg after 3000 epochs: 0.071043\n",
      "loss avg after 3100 epochs: 0.128081\n",
      "loss avg after 3200 epochs: 0.0705925\n",
      "loss avg after 3300 epochs: 0.0537306\n",
      "loss avg after 3400 epochs: 0.110378\n",
      "loss avg after 3500 epochs: 0.213932\n",
      "loss avg after 3600 epochs: 0.0553494\n",
      "loss avg after 3700 epochs: 0.0522914\n",
      "loss avg after 3800 epochs: 0.0571592\n",
      "loss avg after 3900 epochs: 0.102356\n",
      "loss avg after 4000 epochs: 0.0281953\n",
      "loss avg after 4100 epochs: 0.0767833\n",
      "loss avg after 4200 epochs: 0.0538897\n",
      "loss avg after 4300 epochs: 0.052949\n",
      "loss avg after 4400 epochs: 0.503453\n",
      "loss avg after 4500 epochs: 0.0500758\n",
      "loss avg after 4600 epochs: 0.100325\n",
      "loss avg after 4700 epochs: 0.0470766\n",
      "loss avg after 4800 epochs: 0.0347927\n",
      "loss avg after 4900 epochs: 0.0152479\n",
      "loss avg after 5000 epochs: 0.0290831\n",
      "loss avg after 5100 epochs: 0.0137703\n",
      "loss avg after 5200 epochs: 0.00901583\n",
      "loss avg after 5300 epochs: 0.0285817\n",
      "loss avg after 5400 epochs: 0.0220006\n",
      "loss avg after 5500 epochs: 0.0598163\n",
      "loss avg after 5600 epochs: 0.066595\n",
      "loss avg after 5700 epochs: 0.0129715\n",
      "loss avg after 5800 epochs: 0.0119237\n",
      "loss avg after 5900 epochs: 0.0178011\n",
      "loss avg after 6000 epochs: 0.0268047\n",
      "loss avg after 6100 epochs: 0.0088305\n",
      "loss avg after 6200 epochs: 0.00538232\n",
      "loss avg after 6300 epochs: 0.0341328\n",
      "loss avg after 6400 epochs: 0.0139397\n",
      "loss avg after 6500 epochs: 0.01004\n",
      "loss avg after 6600 epochs: 0.00992699\n",
      "loss avg after 6700 epochs: 0.00564013\n",
      "loss avg after 6800 epochs: 0.39052\n",
      "loss avg after 6900 epochs: 0.0379864\n",
      "loss avg after 7000 epochs: 0.0106553\n",
      "loss avg after 7100 epochs: 0.0211918\n",
      "loss avg after 7200 epochs: 0.0286679\n",
      "loss avg after 7300 epochs: 0.0114384\n",
      "loss avg after 7400 epochs: 0.0264551\n",
      "loss avg after 7500 epochs: 0.00944932\n",
      "loss avg after 7600 epochs: 0.0104152\n",
      "loss avg after 7700 epochs: 0.00388225\n",
      "loss avg after 7800 epochs: 0.0139498\n",
      "loss avg after 7900 epochs: 0.00290023\n",
      "loss avg after 8000 epochs: 0.00275621\n",
      "loss avg after 8100 epochs: 0.0539721\n",
      "loss avg after 8200 epochs: 0.0603388\n",
      "loss avg after 8300 epochs: 0.00680452\n",
      "loss avg after 8400 epochs: 0.0158702\n",
      "loss avg after 8500 epochs: 0.00494081\n",
      "loss avg after 8600 epochs: 0.00168057\n",
      "loss avg after 8700 epochs: 0.0050244\n",
      "loss avg after 8800 epochs: 0.00296908\n",
      "loss avg after 8900 epochs: 0.00126851\n",
      "loss avg after 9000 epochs: 0.00209397\n",
      "loss avg after 9100 epochs: 0.0109795\n",
      "loss avg after 9200 epochs: 0.0165698\n",
      "loss avg after 9300 epochs: 0.00564258\n",
      "loss avg after 9400 epochs: 0.00192044\n",
      "accuracy: 1\n",
      "--- repeat 4 of 5 ---\n",
      "loss avg after 100 epochs: 19.7651\n",
      "loss avg after 200 epochs: 10.1729\n",
      "loss avg after 300 epochs: 3.54168\n",
      "loss avg after 400 epochs: 3.66968\n",
      "loss avg after 500 epochs: 3.02427\n",
      "loss avg after 600 epochs: 3.19864\n",
      "loss avg after 700 epochs: 2.11563\n",
      "loss avg after 800 epochs: 1.79778\n",
      "loss avg after 900 epochs: 1.8707\n",
      "loss avg after 1000 epochs: 1.30841\n",
      "loss avg after 1100 epochs: 1.20266\n",
      "loss avg after 1200 epochs: 0.99973\n",
      "loss avg after 1300 epochs: 0.92939\n",
      "loss avg after 1400 epochs: 0.675118\n",
      "loss avg after 1500 epochs: 0.866627\n",
      "loss avg after 1600 epochs: 0.589253\n",
      "loss avg after 1700 epochs: 0.446457\n",
      "loss avg after 1800 epochs: 0.578795\n",
      "loss avg after 1900 epochs: 0.336748\n",
      "loss avg after 2000 epochs: 0.43244\n",
      "loss avg after 2100 epochs: 0.37163\n",
      "loss avg after 2200 epochs: 0.349639\n",
      "loss avg after 2300 epochs: 0.505519\n",
      "loss avg after 2400 epochs: 0.308263\n",
      "loss avg after 2500 epochs: 0.234216\n",
      "loss avg after 2600 epochs: 0.240522\n",
      "loss avg after 2700 epochs: 0.211182\n",
      "loss avg after 2800 epochs: 0.199346\n",
      "loss avg after 2900 epochs: 0.239789\n",
      "loss avg after 3000 epochs: 0.204908\n",
      "loss avg after 3100 epochs: 0.130793\n",
      "loss avg after 3200 epochs: 0.131769\n",
      "loss avg after 3300 epochs: 0.176114\n",
      "loss avg after 3400 epochs: 0.0970842\n",
      "loss avg after 3500 epochs: 0.0894594\n",
      "loss avg after 3600 epochs: 0.107182\n",
      "loss avg after 3700 epochs: 0.0735049\n",
      "loss avg after 3800 epochs: 0.0712431\n",
      "loss avg after 3900 epochs: 0.117307\n",
      "loss avg after 4000 epochs: 0.109836\n",
      "loss avg after 4100 epochs: 0.0480434\n",
      "loss avg after 4200 epochs: 0.0868621\n",
      "loss avg after 4300 epochs: 0.0877889\n",
      "loss avg after 4400 epochs: 0.144306\n",
      "loss avg after 4500 epochs: 0.04415\n",
      "loss avg after 4600 epochs: 0.0453553\n",
      "loss avg after 4700 epochs: 0.0325366\n",
      "loss avg after 4800 epochs: 0.255823\n",
      "loss avg after 4900 epochs: 0.0766292\n",
      "loss avg after 5000 epochs: 0.024509\n",
      "loss avg after 5100 epochs: 0.0323839\n",
      "loss avg after 5200 epochs: 0.0520325\n",
      "loss avg after 5300 epochs: 0.0241424\n",
      "loss avg after 5400 epochs: 0.0418674\n",
      "loss avg after 5500 epochs: 0.0261797\n",
      "loss avg after 5600 epochs: 0.0187519\n",
      "loss avg after 5700 epochs: 0.0160753\n",
      "loss avg after 5800 epochs: 0.0439642\n",
      "loss avg after 5900 epochs: 0.0116634\n",
      "loss avg after 6000 epochs: 0.0173215\n",
      "loss avg after 6100 epochs: 0.0108879\n",
      "loss avg after 6200 epochs: 0.0472428\n",
      "loss avg after 6300 epochs: 0.00558686\n",
      "loss avg after 6400 epochs: 0.0609411\n",
      "loss avg after 6500 epochs: 0.0195413\n",
      "loss avg after 6600 epochs: 0.0191254\n",
      "loss avg after 6700 epochs: 0.0151982\n",
      "loss avg after 6800 epochs: 0.421304\n",
      "loss avg after 6900 epochs: 0.0589965\n",
      "loss avg after 7000 epochs: 0.044707\n",
      "loss avg after 7100 epochs: 0.0332465\n",
      "loss avg after 7200 epochs: 0.0238116\n",
      "loss avg after 7300 epochs: 0.0160482\n",
      "loss avg after 7400 epochs: 0.0111314\n",
      "loss avg after 7500 epochs: 0.00874531\n",
      "loss avg after 7600 epochs: 0.014792\n",
      "loss avg after 7700 epochs: 0.00528673\n",
      "loss avg after 7800 epochs: 0.0480497\n",
      "loss avg after 7900 epochs: 0.0779065\n",
      "loss avg after 8000 epochs: 0.0471563\n",
      "loss avg after 8100 epochs: 0.0107919\n",
      "loss avg after 8200 epochs: 0.00278759\n",
      "loss avg after 8300 epochs: 0.00930856\n",
      "loss avg after 8400 epochs: 0.00250194\n",
      "loss avg after 8500 epochs: 0.00340391\n",
      "loss avg after 8600 epochs: 0.00293085\n",
      "loss avg after 8700 epochs: 0.00183154\n",
      "loss avg after 8800 epochs: 0.0172622\n",
      "loss avg after 8900 epochs: 0.12021\n",
      "loss avg after 9000 epochs: 0.0141236\n",
      "loss avg after 9100 epochs: 0.00967467\n",
      "loss avg after 9200 epochs: 0.0156575\n",
      "loss avg after 9300 epochs: 0.0393979\n",
      "loss avg after 9400 epochs: 0.00653739\n",
      "loss avg after 9500 epochs: 0.00525758\n",
      "loss avg after 9600 epochs: 0.0128862\n",
      "loss avg after 9700 epochs: 0.00193759\n",
      "loss avg after 9800 epochs: 0.0156435\n",
      "loss avg after 9900 epochs: 0.0136216\n",
      "loss avg after 10000 epochs: 0.00140282\n",
      "loss avg after 10100 epochs: 0.0259013\n",
      "loss avg after 10200 epochs: 0.00122251\n",
      "accuracy: 1\n",
      "--- repeat 5 of 5 ---\n",
      "loss avg after 100 epochs: 20.7131\n",
      "loss avg after 200 epochs: 8.69081\n",
      "loss avg after 300 epochs: 5.37075\n",
      "loss avg after 400 epochs: 3.96355\n",
      "loss avg after 500 epochs: 3.22681\n",
      "loss avg after 600 epochs: 2.52995\n",
      "loss avg after 700 epochs: 2.99147\n",
      "loss avg after 800 epochs: 2.05874\n",
      "loss avg after 900 epochs: 1.86699\n",
      "loss avg after 1000 epochs: 1.91309\n",
      "loss avg after 1100 epochs: 1.89141\n",
      "loss avg after 1200 epochs: 1.39549\n",
      "loss avg after 1300 epochs: 0.854044\n",
      "loss avg after 1400 epochs: 0.822503\n",
      "loss avg after 1500 epochs: 0.733938\n",
      "loss avg after 1600 epochs: 0.566382\n",
      "loss avg after 1700 epochs: 0.450981\n",
      "loss avg after 1800 epochs: 0.340998\n",
      "loss avg after 1900 epochs: 0.257642\n",
      "loss avg after 2000 epochs: 0.278122\n",
      "loss avg after 2100 epochs: 0.285631\n",
      "loss avg after 2200 epochs: 0.499108\n",
      "loss avg after 2300 epochs: 0.2051\n",
      "loss avg after 2400 epochs: 0.149335\n",
      "loss avg after 2500 epochs: 0.172432\n",
      "loss avg after 2600 epochs: 0.132138\n",
      "loss avg after 2700 epochs: 0.244284\n",
      "loss avg after 2800 epochs: 0.0871123\n",
      "loss avg after 2900 epochs: 0.234255\n",
      "loss avg after 3000 epochs: 0.117606\n",
      "loss avg after 3100 epochs: 0.10671\n",
      "loss avg after 3200 epochs: 3.9607\n",
      "loss avg after 3300 epochs: 0.1955\n",
      "loss avg after 3400 epochs: 0.104549\n",
      "loss avg after 3500 epochs: 0.0519841\n",
      "loss avg after 3600 epochs: 0.0701941\n",
      "loss avg after 3700 epochs: 0.0511338\n",
      "loss avg after 3800 epochs: 0.0475237\n",
      "loss avg after 3900 epochs: 0.0417508\n",
      "loss avg after 4000 epochs: 0.0415656\n",
      "loss avg after 4100 epochs: 0.0543094\n",
      "loss avg after 4200 epochs: 0.0391293\n",
      "loss avg after 4300 epochs: 0.0380502\n",
      "loss avg after 4400 epochs: 0.0554777\n",
      "loss avg after 4500 epochs: 0.0204699\n",
      "loss avg after 4600 epochs: 0.0216255\n",
      "loss avg after 4700 epochs: 0.0522136\n",
      "loss avg after 4800 epochs: 0.0144265\n",
      "loss avg after 4900 epochs: 0.0135338\n",
      "loss avg after 5000 epochs: 0.0432753\n",
      "loss avg after 5100 epochs: 0.0394647\n",
      "loss avg after 5200 epochs: 0.0161917\n",
      "loss avg after 5300 epochs: 0.0177512\n",
      "loss avg after 5400 epochs: 0.0345034\n",
      "loss avg after 5500 epochs: 0.0209538\n",
      "loss avg after 5600 epochs: 0.01558\n",
      "loss avg after 5700 epochs: 0.0131467\n",
      "loss avg after 5800 epochs: 0.152381\n",
      "loss avg after 5900 epochs: 0.0654866\n",
      "loss avg after 6000 epochs: 0.026602\n",
      "loss avg after 6100 epochs: 0.0171776\n",
      "loss avg after 6200 epochs: 0.00616367\n",
      "loss avg after 6300 epochs: 0.00981831\n",
      "loss avg after 6400 epochs: 0.0192728\n",
      "loss avg after 6500 epochs: 0.00995259\n",
      "loss avg after 6600 epochs: 0.0121729\n",
      "loss avg after 6700 epochs: 0.0122786\n",
      "loss avg after 6800 epochs: 0.0106477\n",
      "loss avg after 6900 epochs: 0.0196879\n",
      "loss avg after 7000 epochs: 0.00968324\n",
      "loss avg after 7100 epochs: 0.261566\n",
      "loss avg after 7200 epochs: 0.0145905\n",
      "loss avg after 7300 epochs: 0.0101307\n",
      "loss avg after 7400 epochs: 0.0435694\n",
      "loss avg after 7500 epochs: 0.0108465\n",
      "loss avg after 7600 epochs: 0.0243542\n",
      "loss avg after 7700 epochs: 0.0250406\n",
      "loss avg after 7800 epochs: 0.00356643\n",
      "loss avg after 7900 epochs: 0.0105212\n",
      "loss avg after 8000 epochs: 0.00445016\n",
      "loss avg after 8100 epochs: 0.0141612\n",
      "loss avg after 8200 epochs: 0.00436746\n",
      "loss avg after 8300 epochs: 0.00719685\n",
      "loss avg after 8400 epochs: 0.0261395\n",
      "loss avg after 8500 epochs: 0.00687663\n",
      "loss avg after 8600 epochs: 0.0120532\n",
      "loss avg after 8700 epochs: 0.00421586\n",
      "loss avg after 8800 epochs: 0.00574375\n",
      "loss avg after 8900 epochs: 0.0404985\n",
      "loss avg after 9000 epochs: 0.0114128\n",
      "loss avg after 9100 epochs: 0.00565386\n",
      "loss avg after 9200 epochs: 0.00708626\n",
      "loss avg after 9300 epochs: 0.280212\n",
      "loss avg after 9400 epochs: 0.0231963\n",
      "loss avg after 9500 epochs: 0.00927492\n",
      "loss avg after 9600 epochs: 0.0168266\n",
      "loss avg after 9700 epochs: 0.0102817\n",
      "loss avg after 9800 epochs: 0.00471832\n",
      "loss avg after 9900 epochs: 0.00259759\n",
      "loss avg after 10000 epochs: 0.00926178\n",
      "accuracy: 1\n"
     ]
    }
   ],
   "source": [
    "# learn\n",
    "import edist.tree_utils as tu\n",
    "import boolean_formulae\n",
    "import random\n",
    "\n",
    "# set unique if you want to check on unique dataset\n",
    "# this is for the experiment described in 4.2.3\n",
    "unique = True\n",
    "list_samples = [i for i in range(max_epochs)]\n",
    "\n",
    "for r in range(R):\n",
    "    print('--- repeat %d of %d ---' % (r+1, R))\n",
    "    if unique:\n",
    "        random.shuffle(list_samples)\n",
    "        test_index = list_samples[:100]\n",
    "        train_index = list_samples[100:]\n",
    "        test_set = [lst[index] for index in test_index] #random 100\n",
    "        train_set = [lst[index] for index in train_index]\n",
    "    # instantiate network and optimizer\n",
    "    net = ten.TEN(num_layers = num_layers, alphabet = boolean_formulae.alphabet,\n",
    "                  dim_hid = dim_hid, skip_connections = skip_connections, nonlin = nonlin,\n",
    "                  dim_in_extra = max_degree + 1)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    # start training\n",
    "    loss_avg = None\n",
    "    learning_curve = []\n",
    "    epochs = 0\n",
    "    while epochs < max_epochs-100:\n",
    "        optimizer.zero_grad()\n",
    "        # sample a nontrivial time series\n",
    "        if unique:\n",
    "            time_series = train_set[epochs]\n",
    "        else:\n",
    "            time_series = boolean_formulae.generate_time_series()\n",
    "        if len(time_series) < 2:\n",
    "            continue\n",
    "        # compute the prediction loss\n",
    "        loss = boolean_formulae.compute_loss(net, time_series)\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # perform an optimizer step\n",
    "        optimizer.step()\n",
    "        # compute a new moving average over the loss\n",
    "        if loss_avg is None:\n",
    "            loss_avg = loss.item()\n",
    "        else:\n",
    "            loss_avg = loss_avg * 0.9 + 0.1 * loss.item()\n",
    "        learning_curve.append(loss.item())\n",
    "        if((epochs+1) % 10 == 0):\n",
    "            print('loss avg after %d epochs: %g' % (epochs+1, loss_avg))\n",
    "        epochs += 1\n",
    "        if loss_avg < loss_threshold:\n",
    "            break\n",
    "\n",
    "\n",
    "    # after training is completed, evaluate\n",
    "    j = 0\n",
    "    T = 0\n",
    "    while j < N_test:\n",
    "        # sample a random time series\n",
    "        if unique:\n",
    "            time_series = test_set[j]\n",
    "        else:\n",
    "            time_series = boolean_formulae.generate_time_series()\n",
    "        if len(time_series) < 2:\n",
    "            continue\n",
    "        # iterate over the time series\n",
    "        for t in range(len(time_series)-1):\n",
    "            # perform the prediction\n",
    "            nodes, adj = time_series[t]\n",
    "            try:\n",
    "                _, nodes_actual, adj_actual = boolean_formulae.predict_step(net, nodes, adj)\n",
    "                nodes_expected, adj_expected = time_series[t+1]\n",
    "                if nodes_actual == nodes_expected and adj_actual == adj_expected:\n",
    "                    accs[r] += 1\n",
    "                else:\n",
    "                    print('expected tree %s but was actually %s' % (tu.tree_to_string(nodes_expected, adj_expected), tu.tree_to_string(nodes_actual, adj_actual)))\n",
    "            except Exception as ex:\n",
    "                try:\n",
    "                    #print('handling exception 1')\n",
    "                    boolean_formulae.predict_step(net, nodes, adj, verbose = True)\n",
    "                except Exception as ex2:\n",
    "                    #print('handling exception 1')\n",
    "                    pass\n",
    "                print('Exception for input tree %s and network output %s\\nexception was %s' % (tu.tree_to_string(nodes, adj, indent = True, with_indices = True), deltaX, str(ex)))\n",
    "\n",
    "\n",
    "        print('managed to exit!')\n",
    "        T += len(time_series)-1\n",
    "        j += 1\n",
    "        print('at the bottom of while')\n",
    "    accs[r] /= T\n",
    "    print('accuracy: %g' % accs[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.972727 +- 0.0545455\n",
      "Epochs: 9490.4 +- 598.325\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "print('Accuracy: %g +- %g' % (np.mean(accs), np.std(accs)))\n",
    "num_epochs = np.array(list(map(len, learning_curves)))\n",
    "print('Epochs: %g +- %g' % (np.mean(num_epochs), np.std(num_epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "smoothing_steps = 10\n",
    "for r in range(R):\n",
    "    # compute a moving average before visualization\n",
    "    acum = np.cumsum(learning_curves[r])\n",
    "    plt.semilogy((acum[smoothing_steps:] - acum[:-smoothing_steps])/smoothing_steps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Addition\n",
    "\n",
    "We generate an addition formula of at most four numbers in the range 1-3 and then use the Peano addition axiom to compute the addition. In particular, the following four rules apply.\n",
    "\n",
    "1. +(m, 0) for any m can be replaced with m.\n",
    "2. +(m, succ(n)) can be replaced with succ(+(m, n)).\n",
    "3. +(m, n) for n in the range 0-9 can be replaced with +(m, succ(n-1)) where -1 refers to the numeric subtraction of 1.\n",
    "4. succ(n) for n in the range 0-9 can be replaced with n+1 (mod 10 because we don't permit two-digit numbers).\n",
    "\n",
    "A time series arises by applying to a current tree every rule that is applicable until a tree results which is only a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../pytorch_tree_edit_networks')\n",
    "\n",
    "import pytorch_tree_edit_networks as ten\n",
    "\n",
    "# the number of experimental repititions\n",
    "R = 5\n",
    "# the number of test trees\n",
    "N_test = 10\n",
    "\n",
    "# training hyperparameters\n",
    "max_epochs     = 30000\n",
    "learning_rate  = 1E-3\n",
    "weight_decay   = 1E-5\n",
    "loss_threshold = 1E-3\n",
    "\n",
    "# model hyperparameters\n",
    "# a single layer with sufficient neurons should suffice here, because we only need to\n",
    "# check immediate parents and children to check whether a rule applies\n",
    "num_layers = 2\n",
    "dim_hid = 64\n",
    "skip_connections = False\n",
    "nonlin = torch.nn.ReLU()\n",
    "max_degree = 10\n",
    "\n",
    "accs = np.zeros(R)\n",
    "learning_curves = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst2 = pd.read_pickle(r'results_temp/peano_timseries.pkl')\n",
    "max_epochs2 = len(lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- repeat 1 of 5 ---\n",
      "loss avg after 100 epochs: 736.22\n",
      "loss avg after 200 epochs: 273.217\n",
      "loss avg after 300 epochs: 231.624\n",
      "loss avg after 400 epochs: 156.77\n",
      "loss avg after 500 epochs: 121.577\n",
      "loss avg after 600 epochs: 91.1509\n",
      "loss avg after 700 epochs: 61.8982\n",
      "loss avg after 800 epochs: 37.245\n",
      "loss avg after 900 epochs: 27.258\n",
      "loss avg after 1000 epochs: 16.4706\n",
      "loss avg after 1100 epochs: 10.4801\n",
      "loss avg after 1200 epochs: 8.1877\n",
      "loss avg after 1300 epochs: 4.9265\n",
      "loss avg after 1400 epochs: 4.89651\n",
      "loss avg after 1500 epochs: 4.07132\n",
      "loss avg after 1600 epochs: 2.45311\n",
      "loss avg after 1700 epochs: 1.9755\n",
      "loss avg after 1800 epochs: 1.75859\n",
      "loss avg after 1900 epochs: 1.66838\n",
      "loss avg after 2000 epochs: 1.57153\n",
      "loss avg after 2100 epochs: 1.46939\n",
      "loss avg after 2200 epochs: 0.802323\n",
      "loss avg after 2300 epochs: 0.983539\n",
      "loss avg after 2400 epochs: 0.771816\n",
      "loss avg after 2500 epochs: 0.713363\n",
      "loss avg after 2600 epochs: 0.575836\n",
      "loss avg after 2700 epochs: 0.592982\n",
      "loss avg after 2800 epochs: 0.520507\n",
      "loss avg after 2900 epochs: 0.46361\n",
      "loss avg after 3000 epochs: 0.431293\n",
      "loss avg after 3100 epochs: 0.442571\n",
      "loss avg after 3200 epochs: 0.360494\n",
      "loss avg after 3300 epochs: 0.313642\n",
      "loss avg after 3400 epochs: 0.609491\n",
      "loss avg after 3500 epochs: 0.498262\n",
      "loss avg after 3600 epochs: 0.282666\n",
      "loss avg after 3700 epochs: 0.278026\n",
      "loss avg after 3800 epochs: 0.249567\n",
      "loss avg after 3900 epochs: 0.184483\n",
      "loss avg after 4000 epochs: 0.163864\n",
      "loss avg after 4100 epochs: 0.25304\n",
      "loss avg after 4200 epochs: 0.154251\n",
      "loss avg after 4300 epochs: 0.1452\n",
      "loss avg after 4400 epochs: 0.131634\n",
      "loss avg after 4500 epochs: 0.109259\n",
      "loss avg after 4600 epochs: 0.0915205\n",
      "loss avg after 4700 epochs: 0.100305\n",
      "loss avg after 4800 epochs: 0.101223\n",
      "loss avg after 4900 epochs: 0.0899368\n",
      "loss avg after 5000 epochs: 0.0971515\n",
      "loss avg after 5100 epochs: 0.0632062\n",
      "loss avg after 5200 epochs: 0.0679928\n",
      "loss avg after 5300 epochs: 0.840297\n",
      "loss avg after 5400 epochs: 0.196916\n",
      "loss avg after 5500 epochs: 0.132968\n",
      "loss avg after 5600 epochs: 0.102356\n",
      "loss avg after 5700 epochs: 0.115711\n",
      "loss avg after 5800 epochs: 0.0858147\n",
      "loss avg after 5900 epochs: 0.0753387\n",
      "loss avg after 6000 epochs: 0.063805\n",
      "loss avg after 6100 epochs: 0.0637566\n",
      "loss avg after 6200 epochs: 0.0478649\n",
      "loss avg after 6300 epochs: 0.0529271\n",
      "loss avg after 6400 epochs: 0.0457485\n",
      "loss avg after 6500 epochs: 0.0468063\n",
      "loss avg after 6600 epochs: 0.0402748\n",
      "loss avg after 6700 epochs: 0.0990476\n",
      "loss avg after 6800 epochs: 0.0566658\n",
      "loss avg after 6900 epochs: 0.0985371\n",
      "loss avg after 7000 epochs: 0.0341537\n",
      "loss avg after 7100 epochs: 0.0330669\n",
      "loss avg after 7200 epochs: 0.0298283\n",
      "loss avg after 7300 epochs: 0.0250172\n",
      "loss avg after 7400 epochs: 0.0241115\n",
      "loss avg after 7500 epochs: 4.30229\n",
      "loss avg after 7600 epochs: 0.166052\n",
      "loss avg after 7700 epochs: 0.0953308\n",
      "loss avg after 7800 epochs: 0.0844133\n",
      "loss avg after 7900 epochs: 0.0723729\n",
      "loss avg after 8000 epochs: 0.0683073\n",
      "loss avg after 8100 epochs: 0.0499612\n",
      "loss avg after 8200 epochs: 0.0441687\n",
      "loss avg after 8300 epochs: 0.0412453\n",
      "loss avg after 8400 epochs: 0.0329766\n",
      "loss avg after 8500 epochs: 0.0309515\n",
      "loss avg after 8600 epochs: 0.023793\n",
      "loss avg after 8700 epochs: 0.0220757\n",
      "loss avg after 8800 epochs: 0.0203476\n",
      "loss avg after 8900 epochs: 0.018253\n",
      "loss avg after 9000 epochs: 0.0172848\n",
      "loss avg after 9100 epochs: 0.0194406\n",
      "loss avg after 9200 epochs: 0.0168419\n",
      "loss avg after 9300 epochs: 0.0133118\n",
      "loss avg after 9400 epochs: 0.0124105\n",
      "loss avg after 9500 epochs: 0.0107914\n",
      "loss avg after 9600 epochs: 0.0101164\n",
      "loss avg after 9700 epochs: 0.0125182\n",
      "loss avg after 9800 epochs: 0.0100956\n",
      "loss avg after 9900 epochs: 0.00838615\n",
      "loss avg after 10000 epochs: 0.00918767\n",
      "loss avg after 10100 epochs: 0.0074516\n",
      "loss avg after 10200 epochs: 0.00790902\n",
      "loss avg after 10300 epochs: 0.00732983\n",
      "loss avg after 10400 epochs: 0.00722321\n",
      "loss avg after 10500 epochs: 0.00841767\n",
      "loss avg after 10600 epochs: 0.0053564\n",
      "loss avg after 10700 epochs: 0.00433721\n",
      "loss avg after 10800 epochs: 0.00633081\n",
      "loss avg after 10900 epochs: 0.00491266\n",
      "loss avg after 11000 epochs: 0.0051019\n",
      "loss avg after 11100 epochs: 0.00468193\n",
      "loss avg after 11200 epochs: 0.00373292\n",
      "loss avg after 11300 epochs: 0.00392409\n",
      "loss avg after 11400 epochs: 0.00440829\n",
      "loss avg after 11500 epochs: 0.0174339\n",
      "loss avg after 11600 epochs: 0.00416891\n",
      "loss avg after 11700 epochs: 0.00333194\n",
      "loss avg after 11800 epochs: 0.00355255\n",
      "loss avg after 11900 epochs: 0.00445297\n",
      "loss avg after 12000 epochs: 0.00244252\n",
      "loss avg after 12100 epochs: 0.00237619\n",
      "loss avg after 12200 epochs: 0.00190732\n",
      "loss avg after 12300 epochs: 0.00252985\n",
      "loss avg after 12400 epochs: 0.00199408\n",
      "loss avg after 12500 epochs: 0.00354492\n",
      "loss avg after 12600 epochs: 0.00234408\n",
      "loss avg after 12700 epochs: 0.00162604\n",
      "loss avg after 12800 epochs: 0.00229146\n",
      "loss avg after 12900 epochs: 0.00138474\n",
      "loss avg after 13000 epochs: 0.00378817\n",
      "loss avg after 13100 epochs: 0.00140058\n",
      "loss avg after 13200 epochs: 0.00151215\n",
      "loss avg after 13300 epochs: 0.00408925\n",
      "loss avg after 13400 epochs: 0.00485283\n",
      "accuracy: 1\n",
      "--- repeat 2 of 5 ---\n",
      "loss avg after 100 epochs: 509.139\n",
      "loss avg after 200 epochs: 299.094\n",
      "loss avg after 300 epochs: 177.855\n",
      "loss avg after 400 epochs: 119.183\n",
      "loss avg after 500 epochs: 62.2557\n",
      "loss avg after 600 epochs: 32.4461\n",
      "loss avg after 700 epochs: 25.1351\n",
      "loss avg after 800 epochs: 12.8609\n",
      "loss avg after 900 epochs: 8.74055\n",
      "loss avg after 1000 epochs: 4.55579\n",
      "loss avg after 1100 epochs: 3.91881\n",
      "loss avg after 1200 epochs: 2.8327\n",
      "loss avg after 1300 epochs: 2.40833\n",
      "loss avg after 1400 epochs: 2.27503\n",
      "loss avg after 1500 epochs: 1.16475\n",
      "loss avg after 1600 epochs: 1.04248\n",
      "loss avg after 1700 epochs: 1.0046\n",
      "loss avg after 1800 epochs: 1.03365\n",
      "loss avg after 1900 epochs: 0.700735\n",
      "loss avg after 2000 epochs: 0.577334\n",
      "loss avg after 2100 epochs: 0.685966\n",
      "loss avg after 2200 epochs: 0.432923\n",
      "loss avg after 2300 epochs: 0.539883\n",
      "loss avg after 2400 epochs: 0.442351\n",
      "loss avg after 2500 epochs: 0.346658\n",
      "loss avg after 2600 epochs: 0.376211\n",
      "loss avg after 2700 epochs: 0.29771\n",
      "loss avg after 2800 epochs: 0.24885\n",
      "loss avg after 2900 epochs: 0.230872\n",
      "loss avg after 3000 epochs: 0.167452\n",
      "loss avg after 3100 epochs: 0.191617\n",
      "loss avg after 3200 epochs: 0.149835\n",
      "loss avg after 3300 epochs: 0.137763\n",
      "loss avg after 3400 epochs: 0.130865\n",
      "loss avg after 3500 epochs: 18.2299\n",
      "loss avg after 3600 epochs: 0.544767\n",
      "loss avg after 3700 epochs: 0.346811\n",
      "loss avg after 3800 epochs: 0.269021\n",
      "loss avg after 3900 epochs: 0.213568\n",
      "loss avg after 4000 epochs: 0.183707\n",
      "loss avg after 4100 epochs: 0.137837\n",
      "loss avg after 4200 epochs: 0.126721\n",
      "loss avg after 4300 epochs: 0.120368\n",
      "loss avg after 4400 epochs: 0.0884721\n",
      "loss avg after 4500 epochs: 0.0943242\n",
      "loss avg after 4600 epochs: 0.10099\n",
      "loss avg after 4700 epochs: 0.0996031\n",
      "loss avg after 4800 epochs: 0.0871938\n",
      "loss avg after 4900 epochs: 0.0571793\n",
      "loss avg after 5000 epochs: 0.0562981\n",
      "loss avg after 5100 epochs: 0.0555684\n",
      "loss avg after 5200 epochs: 0.0527646\n",
      "loss avg after 5300 epochs: 0.0459735\n",
      "loss avg after 5400 epochs: 0.0449326\n",
      "loss avg after 5500 epochs: 0.0361798\n",
      "loss avg after 5600 epochs: 0.0590204\n",
      "loss avg after 5700 epochs: 0.0492073\n",
      "loss avg after 5800 epochs: 0.026371\n",
      "loss avg after 5900 epochs: 0.0265001\n",
      "loss avg after 6000 epochs: 0.0297724\n",
      "loss avg after 6100 epochs: 0.0285274\n",
      "loss avg after 6200 epochs: 0.0271364\n",
      "loss avg after 6300 epochs: 0.0222502\n",
      "loss avg after 6400 epochs: 0.0252773\n",
      "loss avg after 6500 epochs: 0.0190983\n",
      "loss avg after 6600 epochs: 0.0188701\n",
      "loss avg after 6700 epochs: 0.0208822\n",
      "loss avg after 6800 epochs: 0.0177864\n",
      "loss avg after 6900 epochs: 0.0129533\n",
      "loss avg after 7000 epochs: 0.123955\n",
      "loss avg after 7100 epochs: 0.03063\n",
      "loss avg after 7200 epochs: 0.017743\n",
      "loss avg after 7300 epochs: 0.0156407\n",
      "loss avg after 7400 epochs: 0.0193026\n",
      "loss avg after 7500 epochs: 0.0152711\n",
      "loss avg after 7600 epochs: 0.0135315\n",
      "loss avg after 7700 epochs: 0.0120329\n",
      "loss avg after 7800 epochs: 0.0145909\n",
      "loss avg after 7900 epochs: 0.015262\n",
      "loss avg after 8000 epochs: 0.00925964\n",
      "loss avg after 8100 epochs: 0.0437615\n",
      "loss avg after 8200 epochs: 0.00691039\n",
      "loss avg after 8300 epochs: 0.0084782\n",
      "loss avg after 8400 epochs: 0.00637616\n",
      "loss avg after 8500 epochs: 0.00514805\n",
      "loss avg after 8600 epochs: 0.00793561\n",
      "loss avg after 8700 epochs: 0.0110454\n",
      "loss avg after 8800 epochs: 0.00407991\n",
      "loss avg after 8900 epochs: 0.00812938\n",
      "loss avg after 9000 epochs: 0.0118966\n",
      "loss avg after 9100 epochs: 7.90375\n",
      "loss avg after 9200 epochs: 0.0986281\n",
      "loss avg after 9300 epochs: 0.0545051\n",
      "loss avg after 9400 epochs: 0.0278495\n",
      "loss avg after 9500 epochs: 0.0251328\n",
      "loss avg after 9600 epochs: 0.0189607\n",
      "loss avg after 9700 epochs: 0.015454\n",
      "loss avg after 9800 epochs: 0.0126854\n",
      "loss avg after 9900 epochs: 0.0102881\n",
      "loss avg after 10000 epochs: 0.0162707\n",
      "loss avg after 10100 epochs: 0.0076159\n",
      "loss avg after 10200 epochs: 0.0113201\n",
      "loss avg after 10300 epochs: 0.00654354\n",
      "loss avg after 10400 epochs: 0.0190825\n",
      "loss avg after 10500 epochs: 0.00636151\n",
      "loss avg after 10600 epochs: 0.00473269\n",
      "loss avg after 10700 epochs: 0.00856292\n",
      "loss avg after 10800 epochs: 0.00475897\n",
      "loss avg after 10900 epochs: 0.00387245\n",
      "loss avg after 11000 epochs: 0.00483375\n",
      "loss avg after 11100 epochs: 0.00476058\n",
      "loss avg after 11200 epochs: 0.00342859\n",
      "loss avg after 11300 epochs: 0.00326564\n",
      "loss avg after 11400 epochs: 0.00291859\n",
      "loss avg after 11500 epochs: 0.00233979\n",
      "loss avg after 11600 epochs: 0.00265561\n",
      "loss avg after 11700 epochs: 0.00194887\n",
      "loss avg after 11800 epochs: 0.00184201\n",
      "loss avg after 11900 epochs: 0.00218004\n",
      "loss avg after 12000 epochs: 0.00454712\n",
      "loss avg after 12100 epochs: 0.00155795\n",
      "loss avg after 12200 epochs: 0.00149036\n",
      "loss avg after 12300 epochs: 0.00128736\n",
      "loss avg after 12400 epochs: 0.0012668\n",
      "loss avg after 12500 epochs: 0.0019279\n",
      "loss avg after 12600 epochs: 0.0049457\n",
      "loss avg after 12700 epochs: 0.00137179\n",
      "loss avg after 12800 epochs: 0.00130267\n",
      "accuracy: 1\n",
      "--- repeat 3 of 5 ---\n",
      "loss avg after 100 epochs: 532.286\n",
      "loss avg after 200 epochs: 330.941\n",
      "loss avg after 300 epochs: 177.533\n",
      "loss avg after 400 epochs: 107.077\n",
      "loss avg after 500 epochs: 68.6678\n",
      "loss avg after 600 epochs: 38.9942\n",
      "loss avg after 700 epochs: 21.0904\n",
      "loss avg after 800 epochs: 13.1532\n",
      "loss avg after 900 epochs: 8.87039\n",
      "loss avg after 1000 epochs: 7.10372\n",
      "loss avg after 1100 epochs: 3.93298\n",
      "loss avg after 1200 epochs: 3.43251\n",
      "loss avg after 1300 epochs: 2.98113\n",
      "loss avg after 1400 epochs: 1.48515\n",
      "loss avg after 1500 epochs: 1.98408\n",
      "loss avg after 1600 epochs: 1.0401\n",
      "loss avg after 1700 epochs: 1.06855\n",
      "loss avg after 1800 epochs: 1.25839\n",
      "loss avg after 1900 epochs: 0.695781\n",
      "loss avg after 2000 epochs: 0.643028\n",
      "loss avg after 2100 epochs: 0.453937\n",
      "loss avg after 2200 epochs: 0.490316\n",
      "loss avg after 2300 epochs: 0.615947\n",
      "loss avg after 2400 epochs: 0.338478\n",
      "loss avg after 2500 epochs: 1.00215\n",
      "loss avg after 2600 epochs: 0.357518\n",
      "loss avg after 2700 epochs: 0.247445\n",
      "loss avg after 2800 epochs: 0.729622\n",
      "loss avg after 2900 epochs: 0.216938\n",
      "loss avg after 3000 epochs: 0.223507\n",
      "loss avg after 3100 epochs: 0.179071\n",
      "loss avg after 3200 epochs: 0.156452\n",
      "loss avg after 3300 epochs: 0.180619\n",
      "loss avg after 3400 epochs: 0.127328\n",
      "loss avg after 3500 epochs: 0.115675\n",
      "loss avg after 3600 epochs: 0.0983956\n",
      "loss avg after 3700 epochs: 0.102103\n",
      "loss avg after 3800 epochs: 0.0865059\n",
      "loss avg after 3900 epochs: 0.0890463\n",
      "loss avg after 4000 epochs: 0.0790226\n",
      "loss avg after 4100 epochs: 0.0699629\n",
      "loss avg after 4200 epochs: 0.0791207\n",
      "loss avg after 4300 epochs: 0.0631853\n",
      "loss avg after 4400 epochs: 0.0567443\n",
      "loss avg after 4500 epochs: 0.0520407\n",
      "loss avg after 4600 epochs: 2.11225\n",
      "loss avg after 4700 epochs: 0.333988\n",
      "loss avg after 4800 epochs: 0.200906\n",
      "loss avg after 4900 epochs: 0.162702\n",
      "loss avg after 5000 epochs: 0.129338\n",
      "loss avg after 5100 epochs: 0.111871\n",
      "loss avg after 5200 epochs: 0.0839652\n",
      "loss avg after 5300 epochs: 0.13632\n",
      "loss avg after 5400 epochs: 0.0687398\n",
      "loss avg after 5500 epochs: 0.05013\n",
      "loss avg after 5600 epochs: 0.0521948\n",
      "loss avg after 5700 epochs: 0.0491891\n",
      "loss avg after 5800 epochs: 0.0396437\n",
      "loss avg after 5900 epochs: 0.0408674\n",
      "loss avg after 6000 epochs: 0.0462873\n",
      "loss avg after 6100 epochs: 0.0299211\n",
      "loss avg after 6200 epochs: 0.0283761\n",
      "loss avg after 6300 epochs: 0.0310748\n",
      "loss avg after 6400 epochs: 0.0215148\n",
      "loss avg after 6500 epochs: 0.0216798\n",
      "loss avg after 6600 epochs: 0.0231193\n",
      "loss avg after 6700 epochs: 0.0206547\n",
      "loss avg after 6800 epochs: 0.0221251\n",
      "loss avg after 6900 epochs: 0.0202161\n",
      "loss avg after 7000 epochs: 0.0237631\n",
      "loss avg after 7100 epochs: 0.0139177\n",
      "loss avg after 7200 epochs: 0.0131736\n",
      "loss avg after 7300 epochs: 0.015533\n",
      "loss avg after 7400 epochs: 0.0307821\n",
      "loss avg after 7500 epochs: 0.0153371\n",
      "loss avg after 7600 epochs: 0.0126151\n",
      "loss avg after 7700 epochs: 0.0118606\n",
      "loss avg after 7800 epochs: 0.0109834\n",
      "loss avg after 7900 epochs: 0.0139739\n",
      "loss avg after 8000 epochs: 0.00900673\n",
      "loss avg after 8100 epochs: 0.00819683\n",
      "loss avg after 8200 epochs: 0.00606465\n",
      "loss avg after 8300 epochs: 0.00808053\n",
      "loss avg after 8400 epochs: 0.0069702\n",
      "loss avg after 8500 epochs: 0.00805261\n",
      "loss avg after 8600 epochs: 0.00564632\n",
      "loss avg after 8700 epochs: 0.0077852\n",
      "loss avg after 8800 epochs: 0.00544837\n",
      "loss avg after 8900 epochs: 0.00509685\n",
      "loss avg after 9000 epochs: 0.0790268\n",
      "loss avg after 9100 epochs: 0.0472595\n",
      "loss avg after 9200 epochs: 0.0226421\n",
      "loss avg after 9300 epochs: 0.0164551\n",
      "loss avg after 9400 epochs: 0.0146311\n",
      "loss avg after 9500 epochs: 0.0109913\n",
      "loss avg after 9600 epochs: 0.0077714\n",
      "loss avg after 9700 epochs: 0.00872542\n",
      "loss avg after 9800 epochs: 0.0097557\n",
      "loss avg after 9900 epochs: 0.00581203\n",
      "loss avg after 10000 epochs: 0.00599466\n",
      "loss avg after 10100 epochs: 0.00576403\n",
      "loss avg after 10200 epochs: 0.00459803\n",
      "loss avg after 10300 epochs: 0.00536405\n",
      "loss avg after 10400 epochs: 0.016075\n",
      "loss avg after 10500 epochs: 0.00496487\n",
      "loss avg after 10600 epochs: 0.00490628\n",
      "loss avg after 10700 epochs: 0.0041916\n",
      "loss avg after 10800 epochs: 0.0286865\n",
      "loss avg after 10900 epochs: 0.0072515\n",
      "loss avg after 11000 epochs: 0.00705264\n",
      "loss avg after 11100 epochs: 0.027372\n",
      "loss avg after 11200 epochs: 0.0036855\n",
      "loss avg after 11300 epochs: 0.00557089\n",
      "loss avg after 11400 epochs: 0.015712\n",
      "loss avg after 11500 epochs: 0.00267525\n",
      "loss avg after 11600 epochs: 0.00231628\n",
      "loss avg after 11700 epochs: 0.00202307\n",
      "loss avg after 11800 epochs: 0.00188963\n",
      "loss avg after 11900 epochs: 0.00265669\n",
      "loss avg after 12000 epochs: 0.0014709\n",
      "loss avg after 12100 epochs: 0.00222054\n",
      "loss avg after 12200 epochs: 0.00151923\n",
      "loss avg after 12300 epochs: 0.00138683\n",
      "loss avg after 12400 epochs: 0.00128643\n",
      "loss avg after 12500 epochs: 0.00143426\n",
      "accuracy: 1\n",
      "--- repeat 4 of 5 ---\n",
      "loss avg after 100 epochs: 614.872\n",
      "loss avg after 200 epochs: 387.71\n",
      "loss avg after 300 epochs: 226.387\n",
      "loss avg after 400 epochs: 182.653\n",
      "loss avg after 500 epochs: 117.697\n",
      "loss avg after 600 epochs: 77.7296\n",
      "loss avg after 700 epochs: 56.0745\n",
      "loss avg after 800 epochs: 30.7273\n",
      "loss avg after 900 epochs: 20.1831\n",
      "loss avg after 1000 epochs: 12.7127\n",
      "loss avg after 1100 epochs: 9.34298\n",
      "loss avg after 1200 epochs: 7.67338\n",
      "loss avg after 1300 epochs: 5.49318\n",
      "loss avg after 1400 epochs: 4.46887\n",
      "loss avg after 1500 epochs: 2.6465\n",
      "loss avg after 1600 epochs: 2.43077\n",
      "loss avg after 1700 epochs: 1.76459\n",
      "loss avg after 1800 epochs: 1.69827\n",
      "loss avg after 1900 epochs: 1.25476\n",
      "loss avg after 2000 epochs: 1.03013\n",
      "loss avg after 2100 epochs: 1.2783\n",
      "loss avg after 2200 epochs: 1.0031\n",
      "loss avg after 2300 epochs: 0.738759\n",
      "loss avg after 2400 epochs: 0.572098\n",
      "loss avg after 2500 epochs: 0.557142\n",
      "loss avg after 2600 epochs: 0.470819\n",
      "loss avg after 2700 epochs: 0.390996\n",
      "loss avg after 2800 epochs: 0.523431\n",
      "loss avg after 2900 epochs: 0.456531\n",
      "loss avg after 3000 epochs: 0.319316\n",
      "loss avg after 3100 epochs: 0.285902\n",
      "loss avg after 3200 epochs: 0.294019\n",
      "loss avg after 3300 epochs: 0.222071\n",
      "loss avg after 3400 epochs: 0.196749\n",
      "loss avg after 3500 epochs: 0.346402\n",
      "loss avg after 3600 epochs: 0.18493\n",
      "loss avg after 3700 epochs: 0.158329\n",
      "loss avg after 3800 epochs: 0.127195\n",
      "loss avg after 3900 epochs: 0.130263\n",
      "loss avg after 4000 epochs: 0.123366\n",
      "loss avg after 4100 epochs: 0.147342\n",
      "loss avg after 4200 epochs: 0.145907\n",
      "loss avg after 4300 epochs: 0.0844969\n",
      "loss avg after 4400 epochs: 0.0943106\n",
      "loss avg after 4500 epochs: 0.124459\n",
      "loss avg after 4600 epochs: 0.392903\n",
      "loss avg after 4700 epochs: 17.4953\n",
      "loss avg after 4800 epochs: 1.45312\n",
      "loss avg after 4900 epochs: 0.400179\n",
      "loss avg after 5000 epochs: 0.401427\n",
      "loss avg after 5100 epochs: 0.238464\n",
      "loss avg after 5200 epochs: 0.228183\n",
      "loss avg after 5300 epochs: 0.161681\n",
      "loss avg after 5400 epochs: 0.0866756\n",
      "loss avg after 5500 epochs: 0.126176\n",
      "loss avg after 5600 epochs: 0.0749362\n",
      "loss avg after 5700 epochs: 0.0752251\n",
      "loss avg after 5800 epochs: 0.0637414\n",
      "loss avg after 5900 epochs: 0.0546469\n",
      "loss avg after 6000 epochs: 0.0445212\n",
      "loss avg after 6100 epochs: 0.0481116\n",
      "loss avg after 6200 epochs: 0.0428619\n",
      "loss avg after 6300 epochs: 0.0553054\n",
      "loss avg after 6400 epochs: 0.0348743\n",
      "loss avg after 6500 epochs: 0.0323182\n",
      "loss avg after 6600 epochs: 0.0282048\n",
      "loss avg after 6700 epochs: 0.0294763\n",
      "loss avg after 6800 epochs: 0.0325737\n",
      "loss avg after 6900 epochs: 0.0224699\n",
      "loss avg after 7000 epochs: 0.0239023\n",
      "loss avg after 7100 epochs: 0.0197792\n",
      "loss avg after 7200 epochs: 0.0224961\n",
      "loss avg after 7300 epochs: 0.0148997\n",
      "loss avg after 7400 epochs: 0.0189482\n",
      "loss avg after 7500 epochs: 0.0164719\n",
      "loss avg after 7600 epochs: 0.0267072\n",
      "loss avg after 7700 epochs: 0.015286\n",
      "loss avg after 7800 epochs: 0.0174489\n",
      "loss avg after 7900 epochs: 0.00992397\n",
      "loss avg after 8000 epochs: 0.0102808\n",
      "loss avg after 8100 epochs: 0.0107222\n",
      "loss avg after 8200 epochs: 0.011104\n",
      "loss avg after 8300 epochs: 0.00777712\n",
      "loss avg after 8400 epochs: 0.00909578\n",
      "loss avg after 8500 epochs: 0.0126719\n",
      "loss avg after 8600 epochs: 0.00672274\n",
      "loss avg after 8700 epochs: 0.00605427\n",
      "loss avg after 8800 epochs: 0.0063525\n",
      "loss avg after 8900 epochs: 0.00978931\n",
      "loss avg after 9000 epochs: 0.00524467\n",
      "loss avg after 9100 epochs: 0.00560291\n",
      "loss avg after 9200 epochs: 0.00532189\n",
      "loss avg after 9300 epochs: 0.00567705\n",
      "loss avg after 9400 epochs: 0.00509276\n",
      "loss avg after 9500 epochs: 0.00996567\n",
      "loss avg after 9600 epochs: 0.00480216\n",
      "loss avg after 9700 epochs: 0.00425839\n",
      "loss avg after 9800 epochs: 0.00350838\n",
      "loss avg after 9900 epochs: 1.41166\n",
      "loss avg after 10000 epochs: 0.0280335\n",
      "loss avg after 10100 epochs: 0.025666\n",
      "loss avg after 10200 epochs: 0.0132075\n",
      "loss avg after 10300 epochs: 0.0104547\n",
      "loss avg after 10400 epochs: 0.00889735\n",
      "loss avg after 10500 epochs: 0.00659351\n",
      "loss avg after 10600 epochs: 0.00811092\n",
      "loss avg after 10700 epochs: 0.00660984\n",
      "loss avg after 10800 epochs: 0.0059695\n",
      "loss avg after 10900 epochs: 0.00446116\n",
      "loss avg after 11000 epochs: 0.00400167\n",
      "loss avg after 11100 epochs: 0.0274539\n",
      "loss avg after 11200 epochs: 0.0036865\n",
      "loss avg after 11300 epochs: 0.00307846\n",
      "loss avg after 11400 epochs: 0.00240782\n",
      "loss avg after 11500 epochs: 0.00280071\n",
      "loss avg after 11600 epochs: 0.00292779\n",
      "loss avg after 11700 epochs: 0.00276632\n",
      "loss avg after 11800 epochs: 0.0145779\n",
      "loss avg after 11900 epochs: 0.00362066\n",
      "loss avg after 12000 epochs: 0.00193058\n",
      "loss avg after 12100 epochs: 0.00278633\n",
      "loss avg after 12200 epochs: 0.00150662\n",
      "loss avg after 12300 epochs: 0.00152666\n",
      "loss avg after 12400 epochs: 0.00154419\n",
      "loss avg after 12500 epochs: 0.00125745\n",
      "loss avg after 12600 epochs: 0.00114829\n",
      "loss avg after 12700 epochs: 0.00128434\n",
      "accuracy: 1\n",
      "--- repeat 5 of 5 ---\n",
      "loss avg after 100 epochs: 537.645\n",
      "loss avg after 200 epochs: 317.679\n",
      "loss avg after 300 epochs: 234.302\n",
      "loss avg after 400 epochs: 130.202\n",
      "loss avg after 500 epochs: 94.2544\n",
      "loss avg after 600 epochs: 46.6122\n",
      "loss avg after 700 epochs: 28.8254\n",
      "loss avg after 800 epochs: 17.4589\n",
      "loss avg after 900 epochs: 10.3078\n",
      "loss avg after 1000 epochs: 9.08091\n",
      "loss avg after 1100 epochs: 4.83735\n",
      "loss avg after 1200 epochs: 5.59549\n",
      "loss avg after 1300 epochs: 3.43776\n",
      "loss avg after 1400 epochs: 2.20788\n",
      "loss avg after 1500 epochs: 2.09602\n",
      "loss avg after 1600 epochs: 1.35691\n",
      "loss avg after 1700 epochs: 1.23681\n",
      "loss avg after 1800 epochs: 1.12193\n",
      "loss avg after 1900 epochs: 0.7279\n",
      "loss avg after 2000 epochs: 0.966272\n",
      "loss avg after 2100 epochs: 0.782532\n",
      "loss avg after 2200 epochs: 0.830199\n",
      "loss avg after 2300 epochs: 0.500963\n",
      "loss avg after 2400 epochs: 0.501846\n",
      "loss avg after 2500 epochs: 0.428701\n",
      "loss avg after 2600 epochs: 0.344527\n",
      "loss avg after 2700 epochs: 0.432192\n",
      "loss avg after 2800 epochs: 0.301642\n",
      "loss avg after 2900 epochs: 2.73234\n",
      "loss avg after 3000 epochs: 2.10272\n",
      "loss avg after 3100 epochs: 0.803212\n",
      "loss avg after 3200 epochs: 0.53701\n",
      "loss avg after 3300 epochs: 0.49162\n",
      "loss avg after 3400 epochs: 0.357975\n",
      "loss avg after 3500 epochs: 0.319823\n",
      "loss avg after 3600 epochs: 0.314895\n",
      "loss avg after 3700 epochs: 0.214172\n",
      "loss avg after 3800 epochs: 0.197248\n",
      "loss avg after 3900 epochs: 0.194907\n",
      "loss avg after 4000 epochs: 0.158831\n",
      "loss avg after 4100 epochs: 0.112496\n",
      "loss avg after 4200 epochs: 0.149214\n",
      "loss avg after 4300 epochs: 0.148839\n",
      "loss avg after 4400 epochs: 0.101259\n",
      "loss avg after 4500 epochs: 0.0896002\n",
      "loss avg after 4600 epochs: 0.0897541\n",
      "loss avg after 4700 epochs: 0.0795873\n",
      "loss avg after 4800 epochs: 0.106044\n",
      "loss avg after 4900 epochs: 0.0655771\n",
      "loss avg after 5000 epochs: 0.0740394\n",
      "loss avg after 5100 epochs: 0.0753376\n",
      "loss avg after 5200 epochs: 0.0505955\n",
      "loss avg after 5300 epochs: 0.0507829\n",
      "loss avg after 5400 epochs: 0.0403445\n",
      "loss avg after 5500 epochs: 0.035251\n",
      "loss avg after 5600 epochs: 0.0385477\n",
      "loss avg after 5700 epochs: 0.0394106\n",
      "loss avg after 5800 epochs: 0.0414595\n",
      "loss avg after 5900 epochs: 0.0272551\n",
      "loss avg after 6000 epochs: 0.0253951\n",
      "loss avg after 6100 epochs: 0.0332911\n",
      "loss avg after 6200 epochs: 0.0268679\n",
      "loss avg after 6300 epochs: 0.0607868\n",
      "loss avg after 6400 epochs: 0.0257792\n",
      "loss avg after 6500 epochs: 0.0213421\n",
      "loss avg after 6600 epochs: 0.0188931\n",
      "loss avg after 6700 epochs: 0.0421342\n",
      "loss avg after 6800 epochs: 0.083143\n",
      "loss avg after 6900 epochs: 0.0182409\n",
      "loss avg after 7000 epochs: 0.0154054\n",
      "loss avg after 7100 epochs: 0.0248769\n",
      "loss avg after 7200 epochs: 0.0313918\n",
      "loss avg after 7300 epochs: 0.0158804\n",
      "loss avg after 7400 epochs: 0.0141168\n",
      "loss avg after 7500 epochs: 0.014191\n",
      "loss avg after 7600 epochs: 0.0151488\n",
      "loss avg after 7700 epochs: 0.223025\n",
      "loss avg after 7800 epochs: 0.0263953\n",
      "loss avg after 7900 epochs: 0.0138344\n",
      "loss avg after 8000 epochs: 0.0154566\n",
      "loss avg after 8100 epochs: 0.011846\n",
      "loss avg after 8200 epochs: 0.0100876\n",
      "loss avg after 8300 epochs: 0.0126389\n",
      "loss avg after 8400 epochs: 0.0125039\n",
      "loss avg after 8500 epochs: 0.00892256\n",
      "loss avg after 8600 epochs: 0.0118406\n",
      "loss avg after 8700 epochs: 0.00640537\n",
      "loss avg after 8800 epochs: 0.00774524\n",
      "loss avg after 8900 epochs: 0.0490639\n",
      "loss avg after 9000 epochs: 0.0665132\n",
      "loss avg after 9100 epochs: 0.00985407\n",
      "loss avg after 9200 epochs: 0.00694582\n",
      "loss avg after 9300 epochs: 0.00659096\n",
      "loss avg after 9400 epochs: 0.00569614\n",
      "loss avg after 9500 epochs: 0.0219063\n",
      "loss avg after 9600 epochs: 0.0044446\n",
      "loss avg after 9700 epochs: 0.013507\n",
      "loss avg after 9800 epochs: 0.512135\n",
      "loss avg after 9900 epochs: 0.0634694\n",
      "loss avg after 10000 epochs: 0.031327\n",
      "loss avg after 10100 epochs: 0.0253474\n",
      "loss avg after 10200 epochs: 0.0191701\n",
      "loss avg after 10300 epochs: 0.0154384\n",
      "loss avg after 10400 epochs: 0.0141696\n",
      "loss avg after 10500 epochs: 0.0121542\n",
      "loss avg after 10600 epochs: 0.0106393\n",
      "loss avg after 10700 epochs: 0.00848183\n",
      "loss avg after 10800 epochs: 0.00915915\n",
      "loss avg after 10900 epochs: 0.00724434\n",
      "loss avg after 11000 epochs: 0.00574889\n",
      "loss avg after 11100 epochs: 0.0122388\n",
      "loss avg after 11200 epochs: 0.00610764\n",
      "loss avg after 11300 epochs: 0.0045373\n",
      "loss avg after 11400 epochs: 0.00380806\n",
      "loss avg after 11500 epochs: 0.00453014\n",
      "loss avg after 11600 epochs: 0.0037229\n",
      "loss avg after 11700 epochs: 0.0032136\n",
      "loss avg after 11800 epochs: 0.003037\n",
      "loss avg after 11900 epochs: 0.00327445\n",
      "loss avg after 12000 epochs: 0.00431985\n",
      "loss avg after 12100 epochs: 0.00350271\n",
      "loss avg after 12200 epochs: 0.00226139\n",
      "loss avg after 12300 epochs: 0.00187991\n",
      "loss avg after 12400 epochs: 0.00192167\n",
      "loss avg after 12500 epochs: 0.00158669\n",
      "loss avg after 12600 epochs: 0.00206361\n",
      "loss avg after 12700 epochs: 0.00182534\n",
      "loss avg after 12800 epochs: 0.00160143\n",
      "loss avg after 12900 epochs: 0.016496\n",
      "loss avg after 13000 epochs: 0.00369387\n",
      "loss avg after 13100 epochs: 0.00677387\n",
      "loss avg after 13200 epochs: 0.00549045\n",
      "loss avg after 13300 epochs: 0.00369727\n",
      "loss avg after 13400 epochs: 0.00228053\n",
      "loss avg after 13500 epochs: 0.00254256\n",
      "loss avg after 13600 epochs: 0.00197099\n",
      "loss avg after 13700 epochs: 0.00276962\n",
      "loss avg after 13800 epochs: 0.00246478\n",
      "loss avg after 13900 epochs: 0.00112928\n",
      "loss avg after 14000 epochs: 0.00107158\n",
      "accuracy: 1\n"
     ]
    }
   ],
   "source": [
    "# learn\n",
    "import edist.tree_utils as tu\n",
    "import peano_addition\n",
    "\n",
    "# set unique if you want to check on unique dataset\n",
    "# this is for the experiment described in 4.2.3\n",
    "unique = True\n",
    "list_samples2 = [i for i in range(max_epochs2)]\n",
    "\n",
    "\n",
    "for r in range(R):\n",
    "    print('--- repeat %d of %d ---' % (r+1, R))\n",
    "\n",
    "    if unique:\n",
    "        random.shuffle(list_samples2)\n",
    "        test_index = list_samples2[:100]\n",
    "        train_index = list_samples2[100:]\n",
    "        test_set = [lst2[index] for index in test_index] #random 100\n",
    "        train_set = [lst2[index] for index in train_index]\n",
    "    # instantiate network and optimizer\n",
    "    net = ten.TEN(num_layers = num_layers, alphabet = peano_addition.alphabet,\n",
    "                  dim_hid = dim_hid, skip_connections = skip_connections, nonlin = nonlin,\n",
    "                  dim_in_extra = max_degree + 1)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    # start training\n",
    "    loss_avg = None\n",
    "    learning_curve = []\n",
    "    epochs = 0\n",
    "    while epochs < max_epochs-100:\n",
    "        optimizer.zero_grad()\n",
    "        # sample a nontrivial time series\n",
    "        if unique:\n",
    "            time_series = train_set[epochs]\n",
    "        else:\n",
    "            time_series = peano_addition.generate_time_series()\n",
    "        if len(time_series) < 2:\n",
    "            continue\n",
    "        # compute the prediction loss\n",
    "        loss = peano_addition.compute_loss(net, time_series)\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # perform an optimizer step\n",
    "        optimizer.step()\n",
    "        # compute a new moving average over the loss\n",
    "        if loss_avg is None:\n",
    "            loss_avg = loss.item()\n",
    "        else:\n",
    "            loss_avg = loss_avg * 0.9 + 0.1 * loss.item()\n",
    "        learning_curve.append(loss.item())\n",
    "        if((epochs+1) % 100 == 0):\n",
    "            print('loss avg after %d epochs: %g' % (epochs+1, loss_avg))\n",
    "        epochs += 1\n",
    "        if loss_avg < loss_threshold:\n",
    "            break\n",
    "    learning_curves.append(learning_curve)\n",
    "    # after training is completed, evaluate\n",
    "    j = 0\n",
    "    T = 0\n",
    "    while j < N_test:\n",
    "        # sample a random time series\n",
    "        if unique:\n",
    "            time_series = test_set[j]\n",
    "        else:\n",
    "            time_series = peano_addition.generate_time_series()\n",
    "        if len(time_series) < 2:\n",
    "            continue\n",
    "        # iterate over the time series\n",
    "        for t in range(len(time_series)-1):\n",
    "            # perform the prediction\n",
    "            nodes, adj = time_series[t]\n",
    "            try:\n",
    "                _, nodes_actual, adj_actual = peano_addition.predict_step(net, nodes, adj)\n",
    "                nodes_expected, adj_expected = time_series[t+1]\n",
    "                if nodes_actual == nodes_expected and adj_actual == adj_expected:\n",
    "                    accs[r] += 1\n",
    "                else:\n",
    "                    print('expected tree %s but was actually %s' % (tu.tree_to_string(nodes_expected, adj_expected), tu.tree_to_string(nodes_actual, adj_actual)))\n",
    "            except Exception as ex:\n",
    "                try:\n",
    "                    peano_addition.predict_step(net, nodes, adj, verbose = True)\n",
    "                except Exception as ex2:\n",
    "                    pass\n",
    "                print('Exception for input tree %s and network output %s\\nexception was %s' % (tu.tree_to_string(nodes, adj, indent = True, with_indices = True), deltaX, str(ex)))\n",
    "        T += len(time_series)-1\n",
    "        j += 1\n",
    "    accs[r] /= T\n",
    "    print('accuracy: %g' % accs[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1 +- 0\n",
      "Epochs: 13148.8 +- 537.318\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "print('Accuracy: %g +- %g' % (np.mean(accs), np.std(accs)))\n",
    "num_epochs = np.array(list(map(len, learning_curves)))\n",
    "print('Epochs: %g +- %g' % (np.mean(num_epochs), np.std(num_epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "smoothing_steps = 10\n",
    "for r in range(R):\n",
    "    # compute a moving average before visualization\n",
    "    acum = np.cumsum(learning_curves[r])\n",
    "    plt.semilogy((acum[smoothing_steps:] - acum[:-smoothing_steps])/smoothing_steps)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
